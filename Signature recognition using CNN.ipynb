{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HEXA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahulXavier/HEXA/blob/master/HEXA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5J3UzH9qLvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For authentication from google drive and to link googe drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEItpc7jbJov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1. Get sample file\n",
        "#2. Read image and display\n",
        "from IPython.display import Image\n",
        "Image(\"/content/drive/My Drive/Colab Notebooks/Project-Sign samples/HEXA.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmhM1leuhLcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "# This cell contains all the references to libraries that are needed to train a convolutional neural network\n",
        "\n",
        "from __future__ import print_function, division  \n",
        "#To support Py2.0's print function  #To correct ambiguous division operator\n",
        "\n",
        "import numpy as np   #for working with arrays\n",
        "import random #to give a random number between 0 and 1\n",
        "import os #The OS module in Python provides functions for creating and removing a directory (folder), fetching its contents, changing and identifying the current directory, etc.\n",
        "import glob #To find files with a certain extension in a library. Eg: To find .jpg files\n",
        "\n",
        "# https://opencv.org/\n",
        "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n",
        "import cv2 #OpenCV with Numpy Support\n",
        "import datetime\n",
        "import pandas as pd #For data analysis\n",
        "import time \n",
        "import h5py #To work with HDF5 format, which stores huge amounts of numerical data #https://www.h5py.org/ to store trained file\n",
        "import csv #To handle CSV files (Excel)\n",
        "\n",
        "# https://docs.scipy.org/doc/scipy-1.2.1/reference/generated/scipy.misc.imresize.html\n",
        "from skimage.transform import resize\n",
        "\n",
        "#from sklearn.cross_validation import KFold, train_test_split #To estimate skill of machine learning model #Split to Training and Testing, to avoid overfitting\n",
        "from sklearn.metrics import log_loss, confusion_matrix #To calculate loss function #Matrix to visualize performance\n",
        "from sklearn.utils import shuffle #shuffle data for better training\n",
        "\n",
        "from PIL import Image, ImageChops, ImageOps  #To work with images #For image editing #For automatic filters on images\n",
        "#PIL = Python Imaging Library\n",
        "\n",
        "import matplotlib.pyplot as plt #To replicate plot function in matlab\n",
        "\n",
        "from keras import backend as K #To choose backend(Tensorflow,Theano,CTNK,etc)\n",
        "from keras.callbacks import EarlyStopping, Callback #To stop training epochs if training stops improving, to prevent overfitting #To view staticstics during training\n",
        "from keras.utils import np_utils #Embed NumPy with Keras\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array #For data augmentation(rotate,resize,shearing etc) #Load image as PIL #To make 3D NumPy array of img\n",
        "from keras import optimizers #Algorithm to update weights\n",
        "from keras.models import Sequential, model_from_json #For layer-by-layer model creation #A type of ANN model\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Activation, Dropout, Flatten, Dense #layers in CNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s-_qFMtMsnM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ea9f9b3-0c92-49a5-b125-de98ef6794df"
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential #import sequential model(layer-by-layer) as comapared to functional model\n",
        "from keras.layers import Conv2D #For convolution on images\n",
        "from keras.layers import MaxPooling2D #For max-poooling on @d feature maps\n",
        "from keras.layers import Flatten #To Convert pooled 2D image to 1D array\n",
        "from keras.layers import Dense #Connect input to every output\n",
        "\n",
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "# make 32 feature detectors with a size of 3x3\n",
        "# choose the input-image's format to be 64x64 with 3 channels     #relu = Rectified Linear Unit\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape=(128, 128, 3), activation=\"relu\"))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2))) #Converts 2*2=4 pixels to 1\n",
        "\n",
        "# Adding a second convolutional layer and second pooling layer\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(activation=\"softmax\", units=128))\n",
        "#classifier.add(Dense(activation=\"sigmoid\", units=8))  #Set number of output nodes here\n",
        "classifier.add(Dense(activation=\"softmax\", units=8))\n",
        "\n",
        "# Compiling the CNN\n",
        "#Optimizer deals with weight updation using loss function. https://medium.com/datadriveninvestor/optimizers-for-training-neural-networks-e0196662e21e\n",
        "#Adam = Adaptive Momentum Estimation\n",
        "#Loss function required to estimate loss https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
        "#Metrics are used to evaluate model #'accuracy' is to display accuracy while training\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# use ImageDataGenerator to preprocess the data\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Augment the data that we have to create more training data\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range=10,\n",
        "                                   shear_range = 0,\n",
        "                                   zoom_range = 0)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "\n",
        "batchsize = 32 #Default 32\n",
        "# Prepare training data\n",
        "# Select path, Input resolution, input resolution, Batch size, class number.\n",
        "# The batch size is a number of samples processed before the model is updated. https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
        "# class_mode selection - https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\n",
        "training_data = train_datagen.flow_from_directory('/content/drive/My Drive/Colab Notebooks/Project-Sign samples/Signature Datasets/Training',\n",
        "                                                 target_size = (128, 128),\n",
        "                                                 batch_size = batchsize,\n",
        "                                                 class_mode = 'categorical')\n",
        "# prepare test data\n",
        "test_data = test_datagen.flow_from_directory('/content/drive/My Drive/Colab Notebooks/Project-Sign samples/Signature Datasets/Testing',\n",
        "                                            target_size = (128, 128),\n",
        "                                            batch_size = batchsize,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "# finally start computation\n",
        "# to improve the model accuracy you can increase the number of steps_per_epoch to e.g. 8000\n",
        "# increase the number of epochs to 5-25\n",
        "# increase the validation steps\n",
        "# this parametters allow for the model to optimize\n",
        "wandb.init(project=\"CNN\")\n",
        "classifier.fit_generator(training_data,\n",
        "                         steps_per_epoch = (800 / batchsize),  #real 4000\n",
        "                         epochs = 20,\n",
        "                         validation_data = test_data,\n",
        "                         validation_steps = 20,\n",
        "                         callbacks=[WandbCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Found 696 images belonging to 8 classes.\n",
            "Found 120 images belonging to 8 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://app.wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: ERROR Not authenticated.  Copy a key from https://app.wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "API Key: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/rahul619xr/CNN\" target=\"_blank\">https://app.wandb.ai/rahul619xr/CNN</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/rahul619xr/CNN/runs/fqkdznk6\" target=\"_blank\">https://app.wandb.ai/rahul619xr/CNN/runs/fqkdznk6</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "25/25 [==============================] - 241s 10s/step - loss: 2.0211 - acc: 0.2550 - val_loss: 1.9611 - val_acc: 0.3750\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 6s 234ms/step - loss: 1.9410 - acc: 0.4096 - val_loss: 1.9166 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 5s 214ms/step - loss: 1.8877 - acc: 0.5709 - val_loss: 1.8633 - val_acc: 0.6167\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 6s 234ms/step - loss: 1.8448 - acc: 0.6517 - val_loss: 1.8231 - val_acc: 0.7167\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 6s 234ms/step - loss: 1.8011 - acc: 0.7512 - val_loss: 1.7871 - val_acc: 0.7250\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 5s 219ms/step - loss: 1.7676 - acc: 0.7125 - val_loss: 1.7530 - val_acc: 0.7250\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 6s 224ms/step - loss: 1.7330 - acc: 0.7383 - val_loss: 1.7224 - val_acc: 0.7250\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 7s 283ms/step - loss: 1.6977 - acc: 0.7296 - val_loss: 1.6879 - val_acc: 0.7250\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 6s 226ms/step - loss: 1.6635 - acc: 0.7413 - val_loss: 1.6560 - val_acc: 0.7250\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 6s 226ms/step - loss: 1.6355 - acc: 0.7292 - val_loss: 1.6409 - val_acc: 0.7083\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 6s 234ms/step - loss: 1.5982 - acc: 0.7496 - val_loss: 1.5993 - val_acc: 0.7250\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 6s 230ms/step - loss: 1.5689 - acc: 0.7383 - val_loss: 1.5685 - val_acc: 0.7250\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 6s 226ms/step - loss: 1.5447 - acc: 0.7375 - val_loss: 1.5327 - val_acc: 0.7333\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 6s 233ms/step - loss: 1.5077 - acc: 0.7479 - val_loss: 1.5026 - val_acc: 0.7417\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 7s 272ms/step - loss: 1.4774 - acc: 0.7449 - val_loss: 1.4831 - val_acc: 0.7250\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 6s 229ms/step - loss: 1.4519 - acc: 0.7462 - val_loss: 1.4597 - val_acc: 0.7250\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 6s 230ms/step - loss: 1.4288 - acc: 0.7437 - val_loss: 1.4284 - val_acc: 0.7333\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 6s 233ms/step - loss: 1.3987 - acc: 0.7328 - val_loss: 1.4181 - val_acc: 0.7083\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 5s 213ms/step - loss: 1.3755 - acc: 0.7466 - val_loss: 1.3721 - val_acc: 0.7333\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 6s 228ms/step - loss: 1.3458 - acc: 0.7530 - val_loss: 1.3479 - val_acc: 0.7333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f39d05cf588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGeAF0A_1CWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To save training data\n",
        "#To save whole data - classifier.save(filepath) , To save just weights without architecture - classifier.save_weights(filepath)\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Project-Sign samples/\")\n",
        "#classifier.save_model('location') , add architecture seperately\n",
        "classifier.save('model1.h5py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrVlPZmfooAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To load training data\n",
        "from keras.models import load_model\n",
        "classifier = load_model('/content/drive/My Drive/Colab Notebooks/Project-Sign samples/model1.h5py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-83WxhbXXTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model testing\n",
        "\n",
        "from IPython.display import Image #Notebook function to display media, here Image #https://www.dev2qa.com/how-to-display-rich-output-media-audio-video-image-etc-in-ipython-jupyter-notebook/\n",
        "#Image('/content/drive/Colab Notebooks/Project-Sign samples/Input images/Reenu.jpg') #To display image\n",
        "\n",
        "# to make predictions\n",
        "import numpy as np #To convert image to no array\n",
        "from keras.preprocessing import image #To work with images(load,save,etc)\n",
        "img_num = input('Enter Image Number: ')\n",
        "input_image = '/content/drive/My Drive/Colab Notebooks/Project-Sign samples/Input images/'+img_num+'.jpg'\n",
        "\n",
        "test_image = image.load_img(input_image, target_size = (128, 128)) #get image with predefined resolution\n",
        "test_image = image.img_to_array(test_image) #Convert image to np array\n",
        "test_image = np.expand_dims(test_image, axis = 0) #To change dimension of image, create 1D array\n",
        "\n",
        "result = classifier.predict(test_image) #Output predicted\n",
        "result = result[0]  #Takes 1st o/p from numpy array\n",
        "result = list(result) #Convert numpy object to list\n",
        "print(result)\n",
        "namelist = ['Reenu','Renie','Sandra Fulgence','Sholia','Shwetha Sandeep','Sweta Satish','Tabitha','Vivek SP']\n",
        "index = result.index(max(result))\n",
        "print('The signature belongs to',namelist[index])\n",
        "\n",
        "imagename = '/content/drive/My Drive/Colab Notebooks/Project-Sign samples/Output Photos/' + str(index) + '.jpg'\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os, sys\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "img1=mpimg.imread(input_image)\n",
        "img2=mpimg.imread(imagename)\n",
        "plt.figure(1)\n",
        "plt.subplot(121)\n",
        "plt.imshow(img1)\n",
        "plt.subplot(122)\n",
        "plt.imshow(img2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv-_VsiWMFRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To convert image to black and white\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "index = input('Enter image number to convert to B&W: ')\n",
        "  \n",
        "originalImage = cv2.imread('/content/drive/My Drive/Colab Notebooks/Project-Sign samples/Input images/'+index+'.jpg',0)\n",
        "  \n",
        "(thresh, blackAndWhiteImage) = cv2.threshold(originalImage, 127, 255, cv2.THRESH_BINARY)\n",
        "cv2_imshow(blackAndWhiteImage)\n",
        "\n",
        "cv2_imshow(originalImage)\n",
        "cv2.imwrite('/content/drive/My Drive/Colab Notebooks/Project-Sign samples/Input images/'+index+'.jpg', blackAndWhiteImage)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
